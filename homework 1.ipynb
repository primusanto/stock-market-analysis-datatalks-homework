{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65baddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas lxml pandas_datareader setuptools plotly yfinance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dab05c",
   "metadata": {},
   "source": [
    "Question 1: [Index] S&P 500 Stocks Added to the Index\n",
    "Which year had the highest number of additions?\n",
    "\n",
    "Using the list of S&P 500 companies from Wikipedia's S&P 500 companies page, download the data including the year each company was added to the index.\n",
    "\n",
    "Hint: you can use pandas.read_html to scrape the data into a DataFrame.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Create a DataFrame with company tickers, names, and the year they were added.\n",
    "Extract the year from the addition date and calculate the number of stocks added each year.\n",
    "Which year had the highest number of additions (1957 doesn't count, as it was the year when the S&P 500 index was founded)? Write down this year as your answer (the most recent one, if you have several records).\n",
    "Context:\n",
    "\n",
    "\"Following the announcement, all four new entrants saw their stock prices rise in extended trading on Friday\" - recent examples of S&P 500 additions include DASH, WSM, EXE, TKO in 2025 (Nasdaq article).\n",
    "\n",
    "Additional: How many current S&P 500 stocks have been in the index for more than 20 years? When stocks are added to the S&P 500, they usually experience a price bump as investors and index funds buy shares following the announcement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6705411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Fin Data Sources\n",
    "import yfinance as yf\n",
    "import pandas_datareader as pdr\n",
    "\n",
    "#Data viz\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "import time\n",
    "from datetime import date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c58c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>GICS Sub-Industry</th>\n",
       "      <th>Headquarters Location</th>\n",
       "      <th>Date added</th>\n",
       "      <th>CIK</th>\n",
       "      <th>Founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>Saint Paul, Minnesota</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>66740</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Building Products</td>\n",
       "      <td>Milwaukee, Wisconsin</td>\n",
       "      <td>2017-07-26</td>\n",
       "      <td>91142</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott Laboratories</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>1800</td>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Biotechnology</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1551152</td>\n",
       "      <td>2013 (1888)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>IT Consulting &amp; Other Services</td>\n",
       "      <td>Dublin, Ireland</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>1467373</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol             Security             GICS Sector  \\\n",
       "0    MMM                   3M             Industrials   \n",
       "1    AOS          A. O. Smith             Industrials   \n",
       "2    ABT  Abbott Laboratories             Health Care   \n",
       "3   ABBV               AbbVie             Health Care   \n",
       "4    ACN            Accenture  Information Technology   \n",
       "\n",
       "                GICS Sub-Industry    Headquarters Location  Date added  \\\n",
       "0        Industrial Conglomerates    Saint Paul, Minnesota  1957-03-04   \n",
       "1               Building Products     Milwaukee, Wisconsin  2017-07-26   \n",
       "2           Health Care Equipment  North Chicago, Illinois  1957-03-04   \n",
       "3                   Biotechnology  North Chicago, Illinois  2012-12-31   \n",
       "4  IT Consulting & Other Services          Dublin, Ireland  2011-07-06   \n",
       "\n",
       "       CIK      Founded  \n",
       "0    66740         1902  \n",
       "1    91142         1916  \n",
       "2     1800         1888  \n",
       "3  1551152  2013 (1888)  \n",
       "4  1467373         1989  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Scrape the S&P 500 companies table from Wikipedia\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "tables = pd.read_html(url)\n",
    "sp500_df = tables[0]  # The first table contains the list of S&P 500 companies\n",
    "\n",
    "# Display the first few rows\n",
    "sp500_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6112f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_df['Year added'] = pd.to_datetime(sp500_df['Date added'], errors='coerce').dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6effa286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data valid\n"
     ]
    }
   ],
   "source": [
    "#quick check \n",
    "year_added = sp500_df['Year added']\n",
    "valid_years = year_added.dropna()\n",
    "valid_years = valid_years[(valid_years >= 1900) & (valid_years <= 2025)]\n",
    "if len(valid_years) == len(sp500_df):\n",
    "    print(\"data valid\")\n",
    "else:\n",
    "    print(\"check your data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ce99ddc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Year had the highest number of addition\n",
      "A: 2017\n"
     ]
    }
   ],
   "source": [
    "# Count the number of companies added each year (excluding 1957)\n",
    "year_counts = sp500_df[sp500_df['Year added'] != 1957]['Year added'].value_counts()\n",
    "\n",
    "# Get the top years with the most additions\n",
    "year_counts.nlargest(1)\n",
    "print(\"Q: Year had the highest number of addition\\nA:\", year_counts.nlargest(1).index[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0d09eb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How many current S&P 500 stocks have been in the index for more than 20 years?\n",
      "A: 215\n"
     ]
    }
   ],
   "source": [
    "#add the number of years\n",
    "sp500_df['Years in sp500'] = abs((datetime.today() - pd.to_datetime(sp500_df['Date added'], errors='coerce')).dt.days) // 365\n",
    "\n",
    "# Count the number of companies been index for more than 20 years\n",
    "more_than_20_years = sp500_df[sp500_df['Years in sp500'] > 20]\n",
    "print(\"Q: How many current S&P 500 stocks have been in the index for more than 20 years?\\nA:\", len(more_than_20_years))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482b50fc",
   "metadata": {},
   "source": [
    "Question 2. [Macro] Indexes YTD (as of 1 May 2025)\n",
    "How many indexes (out of 10) have better year-to-date returns than the US (S&P 500) as of May 1, 2025?\n",
    "\n",
    "Using Yahoo Finance World Indices data, compare the year-to-date (YTD) performance (1 January-1 May 2025) of major stock market indexes for the following countries:\n",
    "\n",
    "United States - S&P 500 (^GSPC)\n",
    "China - Shanghai Composite (000001.SS)\n",
    "Hong Kong - HANG SENG INDEX (^HSI)\n",
    "Australia - S&P/ASX 200 (^AXJO)\n",
    "India - Nifty 50 (^NSEI)\n",
    "Canada - S&P/TSX Composite (^GSPTSE)\n",
    "Germany - DAX (^GDAXI)\n",
    "United Kingdom - FTSE 100 (^FTSE)\n",
    "Japan - Nikkei 225 (^N225)\n",
    "Mexico - IPC Mexico (^MXX)\n",
    "Brazil - Ibovespa (^BVSP)\n",
    "Hint: use start_date='2025-01-01' and end_date='2025-05-01' when downloading daily data in yfinance\n",
    "\n",
    "Context:\n",
    "\n",
    "Global Valuations: Who's Cheap, Who's Not? article suggests \"Other regions may be growing faster than the US and you need to diversify.\"\n",
    "\n",
    "Reference: Yahoo Finance World Indices - https://finance.yahoo.com/world-indices/\n",
    "\n",
    "Additional: How many of these indexes have better returns than the S&P 500 over 3, 5, and 10 year periods? Do you see the same trend? Note: For simplicity, ignore currency conversion effects.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2dac8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "\n",
    "def get_report_by_date(data,ticker_to_name, start=(date.today() - timedelta(days=1)).strftime(\"%Y-%m-%d\"), end=date.today().strftime(\"%Y-%m-%d\")):\n",
    "    #set the start date jan\n",
    "    print(f'Period for indexes: {start} to {end} ')\n",
    "    data_prepped = data[(data.index >= start) & (data.index <= end)]\n",
    "\n",
    "    # Rename columns in data for easier reading\n",
    "    data_renamed = data_prepped.rename(columns=ticker_to_name)\n",
    "\n",
    "    #data_renamed.head()\n",
    "\n",
    "    # Calculate YTD return for each index\n",
    "    ytd_returns = (data_renamed.iloc[-1] - data_renamed.iloc[0]) / data_renamed.iloc[0] * 100\n",
    "\n",
    "    # Sort by return descending\n",
    "    ytd_returns_sorted = ytd_returns.sort_values(ascending=False)\n",
    "    ytd_returns_ranked = ytd_returns_sorted.reset_index()\n",
    "    ytd_returns_ranked.columns = ['Index', 'YTD Return (%)']\n",
    "    ytd_returns_ranked['Rank'] = ytd_returns_ranked.index + 1\n",
    "    ytd_returns_ranked = ytd_returns_ranked[['Rank', 'Index', 'YTD Return (%)']]\n",
    "    ytd_returns_ranked['YTD Return (%)'] = ytd_returns_ranked['YTD Return (%)'].map('{:.2f}%'.format)\n",
    "    print(ytd_returns_ranked.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "db2d3ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  11 of 11 completed\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "#get data from yahoo finance\n",
    "index_tickers = {\n",
    "    \"United States - S&P 500\": \"^GSPC\",\n",
    "    \"China - Shanghai Composite\": \"000001.SS\",\n",
    "    \"Hong Kong - HANG SENG INDEX\": \"^HSI\",\n",
    "    \"Australia - S&P/ASX 200\": \"^AXJO\",\n",
    "    \"India - Nifty 50\": \"^NSEI\",\n",
    "    \"Canada - S&P/TSX Composite\": \"^GSPTSE\",\n",
    "    \"Germany - DAX\": \"^GDAXI\",\n",
    "    \"United Kingdom - FTSE 100\": \"^FTSE\",\n",
    "    \"Japan - Nikkei 225\": \"^N225\",\n",
    "    \"Mexico - IPC Mexico\": \"^MXX\",\n",
    "    \"Brazil - Ibovespa\": \"^BVSP\"\n",
    "}\n",
    "\n",
    "tickers = list(index_tickers.values())\n",
    "data = yf.download(tickers=tickers, interval='1d')['Close']\n",
    "\n",
    "#most index closed on the 1st of jan, so using ffill to add extra data\n",
    "data_filled = data.ffill(axis=0)\n",
    "\n",
    "# Reverse the index_tickers dictionary to map ticker to name\n",
    "ticker_to_name = {v: k for k, v in index_tickers.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3f6ade2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period for indexes: 2025-01-01 to 2025-05-01 \n",
      " Rank                       Index YTD Return (%)\n",
      "    1         Mexico - IPC Mexico         13.62%\n",
      "    2               Germany - DAX         13.00%\n",
      "    3           Brazil - Ibovespa         12.29%\n",
      "    4 Hong Kong - HANG SENG INDEX         10.27%\n",
      "    5   United Kingdom - FTSE 100          3.96%\n",
      "    6            India - Nifty 50          2.49%\n",
      "    7  Canada - S&P/TSX Composite          0.27%\n",
      "    8     Australia - S&P/ASX 200         -0.17%\n",
      "    9  China - Shanghai Composite         -2.17%\n",
      "   10     United States - S&P 500         -4.72%\n",
      "   11          Japan - Nikkei 225         -8.63%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get data from 1st jan 2025 to 1st may 2025\n",
    "get_report_by_date(data_filled,ticker_to_name,'2025-01-01','2025-05-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b7276b",
   "metadata": {},
   "source": [
    "Bonus question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1d225531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period for indexes: 2022-06-08 to 2025-06-07 \n",
      " Rank                       Index YTD Return (%)\n",
      "    1               Germany - DAX         68.24%\n",
      "    2            India - Nifty 50         52.87%\n",
      "    3     United States - S&P 500         45.79%\n",
      "    4          Japan - Nikkei 225         33.67%\n",
      "    5  Canada - S&P/TSX Composite         27.11%\n",
      "    6           Brazil - Ibovespa         25.59%\n",
      "    7     Australia - S&P/ASX 200         19.58%\n",
      "    8         Mexico - IPC Mexico         16.54%\n",
      "    9   United Kingdom - FTSE 100         16.40%\n",
      "   10 Hong Kong - HANG SENG INDEX          8.08%\n",
      "   11  China - Shanghai Composite          3.72%\n"
     ]
    }
   ],
   "source": [
    "# 3 years ago\n",
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "three_years_ago = (date.today() - timedelta(days=3*365)).strftime(\"%Y-%m-%d\")\n",
    "get_report_by_date(data_filled,ticker_to_name,three_years_ago,today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8701ea99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period for indexes: 2020-06-08 to 2025-06-07 \n",
      " Rank                       Index YTD Return (%)\n",
      "    1            India - Nifty 50        145.91%\n",
      "    2               Germany - DAX         89.59%\n",
      "    3     United States - S&P 500         85.63%\n",
      "    4  Canada - S&P/TSX Composite         65.44%\n",
      "    5          Japan - Nikkei 225         62.83%\n",
      "    6         Mexico - IPC Mexico         45.32%\n",
      "    7     Australia - S&P/ASX 200         41.96%\n",
      "    8           Brazil - Ibovespa         39.38%\n",
      "    9   United Kingdom - FTSE 100         36.54%\n",
      "   10  China - Shanghai Composite         15.24%\n",
      "   11 Hong Kong - HANG SENG INDEX         -3.97%\n"
     ]
    }
   ],
   "source": [
    "# 5 years ago\n",
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "three_years_ago = (date.today() - timedelta(days=5*365)).strftime(\"%Y-%m-%d\")\n",
    "get_report_by_date(data_filled,ticker_to_name,three_years_ago,today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "327343a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period for indexes: 2015-06-10 to 2025-06-07 \n",
      " Rank                       Index YTD Return (%)\n",
      "    1            India - Nifty 50        207.75%\n",
      "    2     United States - S&P 500        185.03%\n",
      "    3           Brazil - Ibovespa        152.62%\n",
      "    4               Germany - DAX        115.74%\n",
      "    5          Japan - Nikkei 225         88.27%\n",
      "    6  Canada - S&P/TSX Composite         77.51%\n",
      "    7     Australia - S&P/ASX 200         55.44%\n",
      "    8         Mexico - IPC Mexico         30.25%\n",
      "    9   United Kingdom - FTSE 100         29.39%\n",
      "   10 Hong Kong - HANG SENG INDEX        -10.85%\n",
      "   11  China - Shanghai Composite        -33.70%\n"
     ]
    }
   ],
   "source": [
    "# 10 years ago\n",
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "three_years_ago = (date.today() - timedelta(days=10*365)).strftime(\"%Y-%m-%d\")\n",
    "get_report_by_date(data_filled,ticker_to_name,three_years_ago,today)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb5bae",
   "metadata": {},
   "source": [
    "Question 3. [Index] S&P 500 Market Corrections Analysis\n",
    "Calculate the median duration (in days) of significant market corrections in the S&P 500 index.\n",
    "\n",
    "For this task, define a correction as an event when a stock index goes down by more than 5% from the closest all-time high maximum.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Download S&P 500 historical data (1950-present) using yfinance\n",
    "Identify all-time high points (where price exceeds all previous prices)\n",
    "For each pair of consecutive all-time highs, find the minimum price in between\n",
    "Calculate drawdown percentages: (high - low) / high × 100\n",
    "Filter for corrections with at least 5% drawdown\n",
    "Calculate the duration in days for each correction period\n",
    "Determine the 25th, 50th (median), and 75th percentiles for correction durations\n",
    "Context:\n",
    "\n",
    "Investors often wonder about the typical length of market corrections when deciding \"when to buy the dip\" (Reddit discussion).\n",
    "A Wealth of Common Sense - How Often Should You Expect a Stock Market Correction?\n",
    "Hint (use this data to compare with your results): Here is the list of top 10 largest corrections by drawdown:\n",
    "\n",
    "2007-10-09 to 2009-03-09: 56.8% drawdown over 517 days\n",
    "2000-03-24 to 2002-10-09: 49.1% drawdown over 929 days\n",
    "1973-01-11 to 1974-10-03: 48.2% drawdown over 630 days\n",
    "1968-11-29 to 1970-05-26: 36.1% drawdown over 543 days\n",
    "2020-02-19 to 2020-03-23: 33.9% drawdown over 33 days\n",
    "1987-08-25 to 1987-12-04: 33.5% drawdown over 101 days\n",
    "1961-12-12 to 1962-06-26: 28.0% drawdown over 196 days\n",
    "1980-11-28 to 1982-08-12: 27.1% drawdown over 622 days\n",
    "2022-01-03 to 2022-10-12: 25.4% drawdown over 282 days\n",
    "1966-02-09 to 1966-10-07: 22.2% drawdown over 240 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7751ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "178cb945",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correction duration percentiles (in days):\n",
      "0.25    19\n",
      "0.50    34\n",
      "0.75    76\n",
      "Name: duration_days, dtype: int64\n",
      "\n",
      "Median duration (days): 34\n",
      "\n",
      "Top 10 largest corrections by drawdown:\n",
      "        start   min_date        end  drawdown_%  duration_days\n",
      "66 2007-10-09 2009-03-09 2013-03-28   56.886671            517\n",
      "64 2000-03-24 2002-10-09 2007-05-30   49.239002            929\n",
      "26 1973-01-11 1974-10-03 1980-07-17   48.715417            630\n",
      "23 1968-11-29 1970-05-26 1972-03-06   36.296770            543\n",
      "78 2020-02-19 2020-03-23 2020-08-18   33.995720             33\n",
      "39 1987-08-25 1987-12-04 1989-07-26   33.761276            101\n",
      "29 1980-11-28 1982-08-12 1982-11-03   28.312451            622\n",
      "16 1961-12-12 1962-06-26 1963-09-03   27.993398            196\n",
      "81 2022-01-03 2022-10-12 2024-01-19   26.091520            282\n",
      "19 1966-02-09 1966-10-07 1967-05-04   22.391860            240\n"
     ]
    }
   ],
   "source": [
    "# Download S&P 500 historical data (1950-present)\n",
    "sp500 = yf.download(\"^GSPC\", start=\"1950-01-01\")['Close']\n",
    "\n",
    "# Find all-time highs\n",
    "all_time_highs = sp500.cummax()\n",
    "\n",
    "# Find the points where a new all-time high is reached\n",
    "ath_mask = sp500 == all_time_highs\n",
    "ath_dates = sp500[ath_mask].dropna().index #= sp500[ath_mask].index\n",
    "\n",
    "# For each pair of consecutive ATHs, find the minimum price in between\n",
    "corrections = []\n",
    "for i in range(len(ath_dates) - 1):\n",
    "    start = ath_dates[i]\n",
    "    end = ath_dates[i+1]\n",
    "    # Only consider periods where there is at least one day between ATHs\n",
    "    if (end - start).days > 1:\n",
    "        period = sp500.loc[start:end]\n",
    "        min_price = period.min().iloc[0]\n",
    "        min_date = period.idxmin().iloc[0]\n",
    "        max_price = period.max().iloc[0]\n",
    "        drawdown = (max_price - min_price) / max_price * 100\n",
    "        duration = (min_date - start).days\n",
    "        if drawdown >= 5:\n",
    "            corrections.append({\n",
    "                'start': start,\n",
    "                'min_date': min_date,\n",
    "                'end': end,\n",
    "                'drawdown_%': drawdown,\n",
    "                'duration_days': duration\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "corrections_df = pd.DataFrame(corrections)\n",
    "\n",
    "# Calculate percentiles\n",
    "percentiles = corrections_df['duration_days'].quantile([0.25, 0.5, 0.75]).astype(int)\n",
    "print(\"Correction duration percentiles (in days):\")\n",
    "print(percentiles)\n",
    "\n",
    "# Show the median duration\n",
    "print(f\"\\nMedian duration (days): {percentiles.loc[0.5]}\")\n",
    "\n",
    "# Show top 10 largest corrections by drawdown\n",
    "print(\"\\nTop 10 largest corrections by drawdown:\")\n",
    "print(corrections_df.sort_values('drawdown_%', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bbda65",
   "metadata": {},
   "source": [
    "Question 4. [Stocks] Earnings Surprise Analysis for Amazon (AMZN)\n",
    "Calculate the median 2-day percentage change in stock prices following positive earnings surprises days.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Load earnings data from CSV (ha1_Amazon.csv) containing earnings dates, EPS estimates, and actual EPS. Make sure you are using the correct delimiter to read the data, such as in this command python pandas.read_csv(\"ha1_Amazon.csv\", delimiter=';')\n",
    "2. Download complete historical price data using yfinance\n",
    "3. Calculate 2-day percentage changes for all historical dates: for each sequence of 3 consecutive trading days (Day 1, Day 2, Day 3), compute the return as Close_Day3 / Close_Day1 - 1. (Assume Day 2 may correspond to the earnings announcement.)\n",
    "4. Identify positive earnings surprises (where \"actual EPS > estimated EPS\"). Both fields should be present in the file. You should obtain 36 data points for use in the descriptive analysis (median) later.\n",
    "5. Calculate 2-day percentage changes following positive earnings surprises. Show your answer in % (closest number to the 2nd digit): return * 100.0\n",
    "6. (Optional) Compare the median 2-day percentage change for positive surprises vs. all historical dates. Do you see the difference?\n",
    "Context: Earnings announcements, especially when they exceed analyst expectations, can significantly impact stock prices in the short term.\n",
    "\n",
    "Reference: Yahoo Finance earnings calendar - https://finance.yahoo.com/calendar/earnings?symbol=AMZN\n",
    "\n",
    "Additional: Is there a correlation between the magnitude of the earnings surprise and the stock price reaction? Does the market react differently to earnings surprises during bull vs. bear markets?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "eb57c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc20d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load earnings data\n",
    "earnings = pd.read_csv(\"ha1_Amazon.csv\", delimiter=';')\n",
    "earnings['test_date'] = pd.to_datetime(earnings['Earnings Date'], format='mixed')\n",
    "earnings['Date'] = pd.to_datetime(earnings['test_date'].dt.strftime('%Y-%m-%d'))\n",
    "\n",
    "\n",
    "# 2. Download historical price data\n",
    "amzn = yf.download(\"AMZN\",interval='1d',period='max')\n",
    "amzn = amzn[['Close']].reset_index()\n",
    "amzn['Date'] = pd.to_datetime(amzn['Date'])\n",
    "amzn.columns = amzn.columns.droplevel(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d866462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(117, 7060)"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "len(earnings), len(amzn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd5dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(amzn, earnings, on='Date', how='left')\n",
    "merged_data['2_day_return'] = 0.0\n",
    "\n",
    "date_match_df = amzn['Date'].isin(earnings['Date']).reindex(amzn.index, fill_value=False)\n",
    "\n",
    "merged_data['cleaned_actual_eps'] = pd.to_numeric(merged_data['Reported EPS'].str.replace('$', '', regex=False), errors='coerce')\n",
    "merged_data['cleaned_estimate_eps'] = pd.to_numeric(merged_data['EPS Estimate'].str.replace('$', '', regex=False), errors='coerce')\n",
    "merged_data['cleaned_actual_eps'].fillna(0, inplace=True)\n",
    "merged_data['cleaned_estimate_eps'].fillna(0, inplace=True)\n",
    "\n",
    "#calculate the 2-day return around the earnings date\n",
    "for i in range(len(date_match_df)-1):\n",
    "    if date_match_df[i] == True:\n",
    "        close_day = merged_data['Close'][i]\n",
    "        close_day_after_2 = merged_data['Close'][i+2] if i + 2 < len(merged_data) else None     \n",
    "        earning_day_rate = close_day_after_2 / close_day - 1 if close_day is not None else None\n",
    "        merged_data.loc[i,'2_day_return'] = earning_day_rate\n",
    "   \n",
    "    i+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fa243c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7060"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check \n",
    "len(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa8ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "positive_earning_surpises = merged_data[merged_data['cleaned_actual_eps'] > merged_data['cleaned_estimate_eps']]\n",
    "# Calculate percentiles\n",
    "percentiles = positive_earning_surpises['2_day_return'].quantile([0.25, 0.5, 0.75]).astype(float)\n",
    "print(\"positive_earning_surpises percentiles:\")\n",
    "print(percentiles*100)\n",
    "\n",
    "# Show the median duration\n",
    "print(f\"\\nMedian positive_earning_surpises: {percentiles.loc[0.5]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "5bf006d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_amzn_data = merged_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "10c00e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive_earning_surpises percentiles:\n",
      "0.25   -1.802157\n",
      "0.50    0.165817\n",
      "0.75    2.151773\n",
      "Name: 2_day_return, dtype: float64\n",
      "\n",
      "Median positive_earning_surpises: 0.16581674487468057\n"
     ]
    }
   ],
   "source": [
    "#calculate the 2-day return around the earnings date for all_amzn_data\n",
    "for i in range(len(date_match_df)-1):\n",
    "    \n",
    "    close_day = all_amzn_data['Close'][i]\n",
    "    close_day_after_2 = all_amzn_data['Close'][i+2] if i + 2 < len(all_amzn_data) else 0  \n",
    "    earning_day_rate = close_day_after_2 / close_day - 1 if close_day is not None else None\n",
    "    all_amzn_data.loc[i,'2_day_return'] = earning_day_rate\n",
    "   \n",
    "    i+= 1\n",
    "\n",
    "# Calculate percentiles\n",
    "percentiles = all_amzn_data['2_day_return'].quantile([0.25, 0.5, 0.75]).astype(float)\n",
    "print(\"positive_earning_surpises percentiles:\")\n",
    "print(percentiles*100)\n",
    "\n",
    "# Show the median duration\n",
    "print(f\"\\nMedian positive_earning_surpises: {percentiles.loc[0.5]*100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e943e62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
